{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"199dYiWC69ER_1WksexyvGvsfT2YxqCkw","timestamp":1715164409181}]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9053005,"sourceType":"datasetVersion","datasetId":5458592}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AlbertTokenizer, AlbertForSequenceClassification #BertTokenizer, BertForSequenceClassification,\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport pandas as pd","metadata":{"id":"rPJdAxrqpvFw","execution":{"iopub.status.busy":"2024-08-25T05:09:09.494844Z","iopub.execute_input":"2024-08-25T05:09:09.495142Z","iopub.status.idle":"2024-08-25T05:09:15.879044Z","shell.execute_reply.started":"2024-08-25T05:09:09.495109Z","shell.execute_reply":"2024-08-25T05:09:15.878081Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/game-reviews/train.csv')\ndf_validation = pd.read_csv('/kaggle/input/game-reviews/validation.csv')\ndf_test = pd.read_csv('/kaggle/input/game-reviews/test.csv')","metadata":{"id":"OcKgDcsQpvFx","execution":{"iopub.status.busy":"2024-08-25T05:09:34.422012Z","iopub.execute_input":"2024-08-25T05:09:34.422567Z","iopub.status.idle":"2024-08-25T05:09:34.986053Z","shell.execute_reply.started":"2024-08-25T05:09:34.422527Z","shell.execute_reply":"2024-08-25T05:09:34.985021Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"BOUvIV1QpvFy","outputId":"8ed419cf-7f73-4a39-caa3-1513f58cfd93","execution":{"iopub.status.busy":"2024-08-25T05:09:37.652635Z","iopub.execute_input":"2024-08-25T05:09:37.653023Z","iopub.status.idle":"2024-08-25T05:09:37.674447Z","shell.execute_reply.started":"2024-08-25T05:09:37.652984Z","shell.execute_reply":"2024-08-25T05:09:37.673570Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   review_id                           title    year  \\\n0        460                     Black Squad  2018.0   \n1       2166   Tree of Savior (English Ver.)  2016.0   \n2      17242               Eternal Card Game  2016.0   \n3       6959  Tactical Monsters Rumble Arena  2018.0   \n4       8807            Yu-Gi-Oh! Duel Links  2017.0   \n\n                                         user_review  user_suggestion  \n0  Early Access ReviewVery great shooter, that ha...                1  \n1  I love love love playing this game!Super 100%!...                1  \n2  Early Access ReviewAs a fan of MTG and Hearths...                1  \n3  Turn based strategy game similiar to FF Tactic...                1  \n4  This game has an insanely huge download for be...                0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>title</th>\n      <th>year</th>\n      <th>user_review</th>\n      <th>user_suggestion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>460</td>\n      <td>Black Squad</td>\n      <td>2018.0</td>\n      <td>Early Access ReviewVery great shooter, that ha...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2166</td>\n      <td>Tree of Savior (English Ver.)</td>\n      <td>2016.0</td>\n      <td>I love love love playing this game!Super 100%!...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17242</td>\n      <td>Eternal Card Game</td>\n      <td>2016.0</td>\n      <td>Early Access ReviewAs a fan of MTG and Hearths...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6959</td>\n      <td>Tactical Monsters Rumble Arena</td>\n      <td>2018.0</td>\n      <td>Turn based strategy game similiar to FF Tactic...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8807</td>\n      <td>Yu-Gi-Oh! Duel Links</td>\n      <td>2017.0</td>\n      <td>This game has an insanely huge download for be...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# # Load pre-trained BERT model and tokenizer\n# model_name = 'bert-base-uncased'\n# tokenizer = BertTokenizer.from_pretrained(model_name)\n# model = BertForSequenceClassification.from_pretrained(model_name)","metadata":{"execution":{"iopub.execute_input":"2024-05-06T09:04:17.579523Z","iopub.status.busy":"2024-05-06T09:04:17.578784Z","iopub.status.idle":"2024-05-06T09:04:17.583458Z","shell.execute_reply":"2024-05-06T09:04:17.582426Z","shell.execute_reply.started":"2024-05-06T09:04:17.579492Z"},"id":"XojmLXN2pvF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'textattack/albert-base-v2-imdb'\ntokenizer = AlbertTokenizer.from_pretrained(model_name)\nmodel = AlbertForSequenceClassification.from_pretrained(model_name)","metadata":{"id":"QBK67nhgX4s2","execution":{"iopub.status.busy":"2024-08-25T05:09:54.376599Z","iopub.execute_input":"2024-08-25T05:09:54.377016Z","iopub.status.idle":"2024-08-25T05:10:00.100674Z","shell.execute_reply.started":"2024-08-25T05:09:54.376978Z","shell.execute_reply":"2024-08-25T05:10:00.099713Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f941fdc738647a7a3fa769a69c3092c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28b09f73eb541a780a316506b20a45d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fdb9db0e84a4b1cac00fe9d767aafa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a05c0b413e498ca127ad5580a26a83"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/46.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fbcd0c514c34af5864267a6d127adcb"}},"metadata":{}}]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts.iloc[idx])\n        label = self.labels.iloc[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"id":"q0HkMTb6pvF0","execution":{"iopub.status.busy":"2024-08-25T05:10:07.396952Z","iopub.execute_input":"2024-08-25T05:10:07.397948Z","iopub.status.idle":"2024-08-25T05:10:07.408357Z","shell.execute_reply.started":"2024-08-25T05:10:07.397906Z","shell.execute_reply":"2024-08-25T05:10:07.406429Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Set maximum sequence length\nMAX_LEN = 128","metadata":{"id":"NLQuPrZbpvF1","execution":{"iopub.status.busy":"2024-08-25T05:10:11.019410Z","iopub.execute_input":"2024-08-25T05:10:11.020163Z","iopub.status.idle":"2024-08-25T05:10:11.026339Z","shell.execute_reply.started":"2024-08-25T05:10:11.020108Z","shell.execute_reply":"2024-08-25T05:10:11.025304Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create DataLoaders for train and val sets\ntrain_dataset = CustomDataset(df_train['user_review'], df_train['user_suggestion'], tokenizer, MAX_LEN)\nval_dataset = CustomDataset(df_validation['user_review'], df_validation['user_suggestion'], tokenizer, MAX_LEN)\ntest_dataset = CustomDataset(df_test['user_review'], df_test['user_suggestion'], tokenizer, MAX_LEN)","metadata":{"id":"YYnma5smpvF2","execution":{"iopub.status.busy":"2024-08-25T05:10:14.385702Z","iopub.execute_input":"2024-08-25T05:10:14.386112Z","iopub.status.idle":"2024-08-25T05:10:14.392053Z","shell.execute_reply.started":"2024-08-25T05:10:14.386074Z","shell.execute_reply":"2024-08-25T05:10:14.391032Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define training parameters\nbatch_size = 32\nepochs = 30\nlr = 2e-5\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","metadata":{"id":"wYJCphGYpvF3","execution":{"iopub.status.busy":"2024-08-25T05:10:57.247571Z","iopub.execute_input":"2024-08-25T05:10:57.248364Z","iopub.status.idle":"2024-08-25T05:10:57.881682Z","shell.execute_reply.started":"2024-08-25T05:10:57.248323Z","shell.execute_reply":"2024-08-25T05:10:57.880947Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"_RjzaXh2pvF4","execution":{"iopub.status.busy":"2024-08-25T05:11:09.858498Z","iopub.execute_input":"2024-08-25T05:11:09.859628Z","iopub.status.idle":"2024-08-25T05:11:09.864835Z","shell.execute_reply.started":"2024-08-25T05:11:09.859583Z","shell.execute_reply":"2024-08-25T05:11:09.863790Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## **Accuracy of Bi-GRU**\n* Training Accuracy: 86.12183252223528%\n* Validation Accuracy: 82.51109370921431%\n* Test Accuracy: 83.24197337509788%","metadata":{"id":"499nlZ6MuJ-D"}},{"cell_type":"code","source":"# Training loop\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)","metadata":{"id":"VmZua7bIpvF4","execution":{"iopub.status.busy":"2024-08-25T05:12:09.351660Z","iopub.execute_input":"2024-08-25T05:12:09.352054Z","iopub.status.idle":"2024-08-25T05:12:09.555239Z","shell.execute_reply.started":"2024-08-25T05:12:09.352020Z","shell.execute_reply":"2024-08-25T05:12:09.554212Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQiZM4RadLP6","outputId":"1b0cce0e-061e-46b7-f011-b2f831b6df3d","execution":{"iopub.status.busy":"2024-08-25T05:12:12.610903Z","iopub.execute_input":"2024-08-25T05:12:12.611289Z","iopub.status.idle":"2024-08-25T05:12:12.617195Z","shell.execute_reply.started":"2024-08-25T05:12:12.611254Z","shell.execute_reply":"2024-08-25T05:12:12.616362Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# Freeze all layers except the classification layer\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the classification layer\nfor param in model.classifier.parameters():\n    param.requires_grad = True","metadata":{"id":"d2cbLwi7pvF5","execution":{"iopub.status.busy":"2024-08-25T05:12:18.012633Z","iopub.execute_input":"2024-08-25T05:12:18.013044Z","iopub.status.idle":"2024-08-25T05:12:18.018459Z","shell.execute_reply.started":"2024-08-25T05:12:18.013004Z","shell.execute_reply":"2024-08-25T05:12:18.017396Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(model, loader, device):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            predictions = torch.argmax(outputs.logits, dim=1)\n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n\n    return 100 * correct / total\n","metadata":{"id":"VgwT17zIuJ-E","execution":{"iopub.status.busy":"2024-08-25T05:12:23.865659Z","iopub.execute_input":"2024-08-25T05:12:23.866685Z","iopub.status.idle":"2024-08-25T05:12:23.873210Z","shell.execute_reply.started":"2024-08-25T05:12:23.866641Z","shell.execute_reply":"2024-08-25T05:12:23.872015Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils import clip_grad_norm_\n\nbest_val_loss = float('inf')  # Initialize best_val_loss to a very high value\nbest_epoch = -1  # Initialize best_epoch to an invalid value to track the epoch of the best validation loss\n\nfor epoch in range(epochs):\n    model.train()\n    total_train_loss = 0\n    total_val_loss = 0\n\n    # Training\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        \n        # Gradient clipping\n        clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n    avg_val_loss = total_val_loss / len(val_loader)\n\n    # Check if the current validation loss is the lowest; if so, save the model\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_epoch = epoch\n        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n\n# Print the best epoch and its validation loss\nprint(f\"The lowest validation loss was {best_val_loss:.4f} at epoch {best_epoch + 1}\")\n\n# Load the best model and calculate accuracy\nmodel.load_state_dict(torch.load('best_model.pth'))\ntrain_accuracy = calculate_accuracy(model, train_loader, device)\nval_accuracy = calculate_accuracy(model, val_loader, device)\n\nprint(f'Best Model Training Accuracy: {train_accuracy:.2f}%')\nprint(f'Best Model Validation Accuracy: {val_accuracy:.2f}%')\n","metadata":{"id":"AiuH5tfiuJ-E","outputId":"10921318-c3af-44a6-a277-e91403d35732","execution":{"iopub.status.busy":"2024-08-25T05:15:07.676877Z","iopub.execute_input":"2024-08-25T05:15:07.677777Z","iopub.status.idle":"2024-08-25T06:25:19.060358Z","shell.execute_reply.started":"2024-08-25T05:15:07.677734Z","shell.execute_reply":"2024-08-25T06:25:19.059364Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/30, Training Loss: 0.4039, Validation Loss: 0.3866\nEpoch 2/30, Training Loss: 0.3940, Validation Loss: 0.3803\nEpoch 3/30, Training Loss: 0.3874, Validation Loss: 0.3760\nEpoch 4/30, Training Loss: 0.3834, Validation Loss: 0.3736\nEpoch 5/30, Training Loss: 0.3802, Validation Loss: 0.3709\nEpoch 6/30, Training Loss: 0.3777, Validation Loss: 0.3684\nEpoch 7/30, Training Loss: 0.3754, Validation Loss: 0.3670\nEpoch 8/30, Training Loss: 0.3740, Validation Loss: 0.3650\nEpoch 9/30, Training Loss: 0.3713, Validation Loss: 0.3631\nEpoch 10/30, Training Loss: 0.3697, Validation Loss: 0.3618\nEpoch 11/30, Training Loss: 0.3692, Validation Loss: 0.3608\nEpoch 12/30, Training Loss: 0.3674, Validation Loss: 0.3591\nEpoch 13/30, Training Loss: 0.3654, Validation Loss: 0.3583\nEpoch 14/30, Training Loss: 0.3653, Validation Loss: 0.3589\nEpoch 15/30, Training Loss: 0.3645, Validation Loss: 0.3564\nEpoch 16/30, Training Loss: 0.3631, Validation Loss: 0.3564\nEpoch 17/30, Training Loss: 0.3617, Validation Loss: 0.3556\nEpoch 18/30, Training Loss: 0.3619, Validation Loss: 0.3552\nEpoch 19/30, Training Loss: 0.3606, Validation Loss: 0.3542\nEpoch 20/30, Training Loss: 0.3604, Validation Loss: 0.3528\nEpoch 21/30, Training Loss: 0.3601, Validation Loss: 0.3533\nEpoch 22/30, Training Loss: 0.3590, Validation Loss: 0.3527\nEpoch 23/30, Training Loss: 0.3587, Validation Loss: 0.3518\nEpoch 24/30, Training Loss: 0.3574, Validation Loss: 0.3512\nEpoch 25/30, Training Loss: 0.3572, Validation Loss: 0.3507\nEpoch 26/30, Training Loss: 0.3567, Validation Loss: 0.3502\nEpoch 27/30, Training Loss: 0.3562, Validation Loss: 0.3499\nEpoch 28/30, Training Loss: 0.3562, Validation Loss: 0.3500\nEpoch 29/30, Training Loss: 0.3556, Validation Loss: 0.3493\nEpoch 30/30, Training Loss: 0.3551, Validation Loss: 0.3488\nThe lowest validation loss was 0.3488 at epoch 30\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1629294865.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Best Model Training Accuracy: 84.81%\nBest Model Validation Accuracy: 84.03%\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('best_model.pth'))\ntest_accuracy = calculate_accuracy(model,test_loader, device)\nprint(f'Test Accuracy: {test_accuracy}%')","metadata":{"id":"L9uS-8TVpvF6","outputId":"b92bab10-fea2-4876-8a14-baca5daf497f","execution":{"iopub.status.busy":"2024-08-25T06:28:18.030435Z","iopub.execute_input":"2024-08-25T06:28:18.030857Z","iopub.status.idle":"2024-08-25T06:28:41.967829Z","shell.execute_reply.started":"2024-08-25T06:28:18.030817Z","shell.execute_reply":"2024-08-25T06:28:41.966854Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2013887257.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.25189245627773%\n","output_type":"stream"}]}]}